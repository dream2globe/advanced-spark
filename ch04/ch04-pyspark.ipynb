{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "## Referrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "# from pytz import timezone\n",
    "# from pytz import utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTOR_MEMORY = \"2g\"\n",
    "EXECUTOR_CORES = 2\n",
    "EXECUTORE_INSTANCES = 3\n",
    "DRIVER_MEMORY = \"1g\"\n",
    "DRIVER_MAX_RESULT_SIZE = \"1g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'desktop'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.appUIAddress', 'http://192.168.0.2:4040'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.driver.host', '192.168.0.2'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://desktop:8088/proxy/application_1611448590783_0003'),\n",
       " ('spark.app.name', 'Advanced analytics with SPARK - Chapter 3'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.history.fs.update.interval', '10s'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs://desktop:9000/spark-logs'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.eventLog.dir', 'hdfs://desktop:9000/spark-logs'),\n",
       " ('spark.executor.memory', '2g'),\n",
       " ('spark.kryoserializer.buffer.max', '1024m'),\n",
       " ('spark.driver.port', '46249'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.driver.maxResultSize', '1g'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1611448590783_0003'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'application_1611448590783_0003')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(f\"Advanced analytics with SPARK - Chapter 3\")\n",
    "    .master(\"yarn\")\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEMORY)\n",
    "    .config(\"spark.executor.cores\", EXECUTOR_CORES)\n",
    "    .config(\"spark.executor.instances\", EXECUTORE_INSTANCES)\n",
    "    .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "    .config(\"spark.driver.maxResultSize\", DRIVER_MAX_RESULT_SIZE)\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "#     .config(\"spark.sql.warehouse.dir\", \"/user/bigdata/members/shyeon/advanced-spark/data\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Forest CoverType dataset\n",
      "\n",
      "\n",
      "1.\tTitle of Database:\n",
      "\n",
      "\tForest Covertype data\n",
      "\n",
      "\n",
      "2.\tSources:\n",
      "\n",
      "\t(a) Original owners of database:\n",
      "\t\tRemote Sensing and GIS Program\n",
      "\t\tDepartment of Forest Sciences\n",
      "\t\tCollege of Natural Resources\n",
      "\t\tColorado State University\n",
      "\t\tFort Collins, CO  80523\n",
      "\t\t(contact Jock A. Blackard, jblackard 'at' fs.fed.us\n",
      "\t\t      or Dr. Denis J. Dean, denis.dean 'at' utdallas.edu)\n",
      "\n",
      "\tNOTE:\tReuse of this database is unlimited with retention of \n",
      "\t\tcopyright notice for Jock A. Blackard and Colorado \n",
      "\t\tState University.\n",
      "\n",
      "\t(b) Donors of database:\n",
      "\t\tJock A. Blackard (jblackard 'at' fs.fed.us)\n",
      "\t\tGIS Coordinator\n",
      "\t\tUSFS - Forest Inventory & Analysis\n",
      "\t\tRocky Mountain Research Station\n",
      "\t\t507 25th Street\n",
      "\t\tOgden, UT 84401\n",
      "\n",
      "\t\tDr. Denis J. Dean (denis.dean 'at' utdallas.edu)\n",
      "\t\tProfessor\n",
      "\t\tProgram in Geography and Geospatial Sciences\n",
      "\t\tSchool of Economic, Political and Policy Sciences\n",
      "\t\t800 West Campbell Rd\n",
      "\t\tRichardson, TX  75080-3021 \n",
      "\t\t\n",
      "\t\tDr. Charles W. Anderson (anderson 'at' cs.colostate.edu)\n",
      "\t\tAssociate Professor\n",
      "\t\tDepartment of Computer Science\n",
      "\t\tColorado State University\n",
      "\t\tFort Collins, CO  80523  USA\n",
      "\n",
      "\t(c) Date donated:  August 1998\n",
      "\n",
      "\n",
      "3.\tPast Usage:\n",
      "\n",
      "\tBlackard, Jock A. and Denis J. Dean.  2000.  \"Comparative\n",
      "\t\tAccuracies of Artificial Neural Networks and Discriminant\n",
      "\t\tAnalysis in Predicting Forest Cover Types from Cartographic\n",
      "\t\tVariables.\"  Computers and Electronics in Agriculture \n",
      "\t\t24(3):131-151.\n",
      "\n",
      "\tBlackard, Jock A. and Denis J. Dean.  1998.  \"Comparative\n",
      "\t\tAccuracies of Neural Networks and Discriminant Analysis\n",
      "\t\tin Predicting Forest Cover Types from Cartographic \n",
      "\t\tVariables.\"  Second Southern Forestry GIS Conference.\n",
      "\t\tUniversity of Georgia.  Athens, GA.  Pages 189-199.\n",
      "\n",
      "\tBlackard, Jock A.  1998.  \"Comparison of Neural Networks and\n",
      "\t\tDiscriminant Analysis in Predicting Forest Cover Types.\"\n",
      "\t\tPh.D. dissertation.  Department of Forest Sciences.  \n",
      "\t\tColorado State University.  Fort Collins, Colorado.  \n",
      "\t\t165 pages.\n",
      "\n",
      "\tAbstract of dissertation:\n",
      "\t\tNatural resource managers responsible for developing \n",
      "\tecosystem management strategies require basic descriptive \n",
      "\tinformation including inventory data for forested lands to \n",
      "\tsupport their decision-making processes.  However, managers \n",
      "\tgenerally do not have this type of data for inholdings or \n",
      "\tneighboring lands that are outside their immediate \n",
      "\tjurisdiction.  One method of obtaining this information is \n",
      "\tthrough the use of predictive models.  \n",
      "\t\tTwo predictive models were examined in this study, a \n",
      "\tfeedforward neural network model and a more traditional \n",
      "\tstatistical model based on discriminant analysis.  The overall \n",
      "\tobjectives of this research were to first construct these two \n",
      "\tpredictive models, and second to compare and evaluate their \n",
      "\trespective classification accuracies when predicting forest \n",
      "\tcover types in undisturbed forests.  \n",
      "\t\tThe study area included four wilderness areas found in \n",
      "\tthe Roosevelt National Forest of northern Colorado.  A total \n",
      "\tof twelve cartographic measures were utilized as independent \n",
      "\tvariables in the predictive models, while seven major forest \n",
      "\tcover types were used as dependent variables.  Several subsets \n",
      "\tof these variables were examined to determine the best overall \n",
      "\tpredictive model.  \n",
      "\t\tFor each subset of cartographic variables examined in \n",
      "\tthis study, relative classification accuracies indicate the \n",
      "\tneural network approach outperformed the traditional \n",
      "\tdiscriminant analysis method in predicting forest cover types.  \n",
      "\tThe final neural network model had a higher absolute \n",
      "\tclassification accuracy (70.58%) than the final corresponding \n",
      "\tlinear discriminant analysis model(58.38%).  In support of these \n",
      "\tclassification results, thirty additional networks with randomly \n",
      "\tselected initial weights were derived.  From these networks, the \n",
      "\toverall mean absolute classification accuracy for the neural \n",
      "\tnetwork method was 70.52%, with a 95% confidence interval of \n",
      "\t70.26% to 70.80%.  Consequently, natural resource managers may \n",
      "\tutilize an alternative method of predicting forest cover types \n",
      "\tthat is both superior to the traditional statistical methods and \n",
      "\tadequate to support their decision-making processes for \n",
      "\tdeveloping ecosystem management strategies.\n",
      "\n",
      "\n",
      "\t-- Classification performance\n",
      "\t\t-- first 11,340 records used for training data subset\n",
      "\t\t-- next 3,780 records used for validation data subset\n",
      "\t\t-- last 565,892 records used for testing data subset\n",
      "\t\t-- 70% Neural Network (backpropagation)\n",
      "\t\t-- 58% Linear Discriminant Analysis\n",
      "\n",
      "\n",
      "4.\tRelevant Information Paragraph:\n",
      "\n",
      "\tPredicting forest cover type from cartographic variables only\n",
      "\t(no remotely sensed data).  The actual forest cover type for\n",
      "\ta given observation (30 x 30 meter cell) was determined from\n",
      "\tUS Forest Service (USFS) Region 2 Resource Information System \n",
      "\t(RIS) data.  Independent variables were derived from data\n",
      "\toriginally obtained from US Geological Survey (USGS) and\n",
      "\tUSFS data.  Data is in raw form (not scaled) and contains\n",
      "\tbinary (0 or 1) columns of data for qualitative independent\n",
      "\tvariables (wilderness areas and soil types).\n",
      "\n",
      "\tThis study area includes four wilderness areas located in the\n",
      "\tRoosevelt National Forest of northern Colorado.  These areas\n",
      "\trepresent forests with minimal human-caused disturbances,\n",
      "\tso that existing forest cover types are more a result of \n",
      "\tecological processes rather than forest management practices.\n",
      "\n",
      "\tSome background information for these four wilderness areas:  \n",
      "\tNeota (area 2) probably has the highest mean elevational value of \n",
      "\tthe 4 wilderness areas. Rawah (area 1) and Comanche Peak (area 3) \n",
      "\twould have a lower mean elevational value, while Cache la Poudre \n",
      "\t(area 4) would have the lowest mean elevational value. \n",
      "\n",
      "\tAs for primary major tree species in these areas, Neota would have \n",
      "\tspruce/fir (type 1), while Rawah and Comanche Peak would probably\n",
      "\thave lodgepole pine (type 2) as their primary species, followed by \n",
      "\tspruce/fir and aspen (type 5). Cache la Poudre would tend to have \n",
      "\tPonderosa pine (type 3), Douglas-fir (type 6), and \n",
      "\tcottonwood/willow (type 4).  \n",
      "\n",
      "\tThe Rawah and Comanche Peak areas would tend to be more typical of \n",
      "\tthe overall dataset than either the Neota or Cache la Poudre, due \n",
      "\tto their assortment of tree species and range of predictive \n",
      "\tvariable values (elevation, etc.)  Cache la Poudre would probably \n",
      "\tbe more unique than the others, due to its relatively low \n",
      "\televation range and species composition. \n",
      "\n",
      "\n",
      "5.\tNumber of instances (observations):  581,012\n",
      "\n",
      "\n",
      "6.\tNumber of Attributes:\t12 measures, but 54 columns of data\n",
      "\t\t\t\t(10 quantitative variables, 4 binary\n",
      "\t\t\t\twilderness areas and 40 binary\n",
      "\t\t\t\tsoil type variables)\n",
      "\n",
      "\n",
      "7.\tAttribute information:\n",
      "\n",
      "Given is the attribute name, attribute type, the measurement unit and\n",
      "a brief description.  The forest cover type is the classification \n",
      "problem.  The order of this listing corresponds to the order of \n",
      "numerals along the rows of the database.\n",
      "\n",
      "Name                                     Data Type    Measurement                       Description\n",
      "\n",
      "Elevation                               quantitative    meters                       Elevation in meters\n",
      "Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
      "Slope                                   quantitative    degrees                      Slope in degrees\n",
      "Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
      "Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
      "Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
      "Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
      "Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
      "Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
      "Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
      "Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
      "Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
      "Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
      "\n",
      "\n",
      "Code Designations:\n",
      "\n",
      "Wilderness Areas:  \t1 -- Rawah Wilderness Area\n",
      "                        2 -- Neota Wilderness Area\n",
      "                        3 -- Comanche Peak Wilderness Area\n",
      "                        4 -- Cache la Poudre Wilderness Area\n",
      "\n",
      "Soil Types:             1 to 40 : based on the USFS Ecological\n",
      "                        Landtype Units (ELUs) for this study area:\n",
      "\n",
      "  Study Code USFS ELU Code\t\t\tDescription\n",
      "\t 1\t   2702\t\tCathedral family - Rock outcrop complex, extremely stony.\n",
      "\t 2\t   2703\t\tVanet - Ratake families complex, very stony.\n",
      "\t 3\t   2704\t\tHaploborolis - Rock outcrop complex, rubbly.\n",
      "\t 4\t   2705\t\tRatake family - Rock outcrop complex, rubbly.\n",
      "\t 5\t   2706\t\tVanet family - Rock outcrop complex complex, rubbly.\n",
      "\t 6\t   2717\t\tVanet - Wetmore families - Rock outcrop complex, stony.\n",
      "\t 7\t   3501\t\tGothic family.\n",
      "\t 8\t   3502\t\tSupervisor - Limber families complex.\n",
      "\t 9\t   4201\t\tTroutville family, very stony.\n",
      "\t10\t   4703\t\tBullwark - Catamount families - Rock outcrop complex, rubbly.\n",
      "\t11\t   4704\t\tBullwark - Catamount families - Rock land complex, rubbly.\n",
      "\t12\t   4744\t\tLegault family - Rock land complex, stony.\n",
      "\t13\t   4758\t\tCatamount family - Rock land - Bullwark family complex, rubbly.\n",
      "\t14\t   5101\t\tPachic Argiborolis - Aquolis complex.\n",
      "\t15\t   5151\t\tunspecified in the USFS Soil and ELU Survey.\n",
      "\t16\t   6101\t\tCryaquolis - Cryoborolis complex.\n",
      "\t17\t   6102\t\tGateview family - Cryaquolis complex.\n",
      "\t18\t   6731\t\tRogert family, very stony.\n",
      "\t19\t   7101\t\tTypic Cryaquolis - Borohemists complex.\n",
      "\t20\t   7102\t\tTypic Cryaquepts - Typic Cryaquolls complex.\n",
      "\t21\t   7103\t\tTypic Cryaquolls - Leighcan family, till substratum complex.\n",
      "\t22\t   7201\t\tLeighcan family, till substratum, extremely bouldery.\n",
      "\t23\t   7202\t\tLeighcan family, till substratum - Typic Cryaquolls complex.\n",
      "\t24\t   7700\t\tLeighcan family, extremely stony.\n",
      "\t25\t   7701\t\tLeighcan family, warm, extremely stony.\n",
      "\t26\t   7702\t\tGranile - Catamount families complex, very stony.\n",
      "\t27\t   7709\t\tLeighcan family, warm - Rock outcrop complex, extremely stony.\n",
      "\t28\t   7710\t\tLeighcan family - Rock outcrop complex, extremely stony.\n",
      "\t29\t   7745\t\tComo - Legault families complex, extremely stony.\n",
      "\t30\t   7746\t\tComo family - Rock land - Legault family complex, extremely stony.\n",
      "\t31\t   7755\t\tLeighcan - Catamount families complex, extremely stony.\n",
      "\t32\t   7756\t\tCatamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
      "\t33\t   7757\t\tLeighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
      "\t34\t   7790\t\tCryorthents - Rock land complex, extremely stony.\n",
      "\t35\t   8703\t\tCryumbrepts - Rock outcrop - Cryaquepts complex.\n",
      "\t36\t   8707\t\tBross family - Rock land - Cryumbrepts complex, extremely stony.\n",
      "\t37\t   8708\t\tRock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
      "\t38\t   8771\t\tLeighcan - Moran families - Cryaquolls complex, extremely stony.\n",
      "\t39\t   8772\t\tMoran family - Cryorthents - Leighcan family complex, extremely stony.\n",
      "\t40\t   8776\t\tMoran family - Cryorthents - Rock land complex, extremely stony.\n",
      "\n",
      "        Note:   First digit:  climatic zone             Second digit:  geologic zones\n",
      "                1.  lower montane dry                   1.  alluvium\n",
      "                2.  lower montane                       2.  glacial\n",
      "                3.  montane dry                         3.  shale\n",
      "                4.  montane                             4.  sandstone\n",
      "                5.  montane dry and montane             5.  mixed sedimentary\n",
      "                6.  montane and subalpine               6.  unspecified in the USFS ELU Survey\n",
      "                7.  subalpine                           7.  igneous and metamorphic\n",
      "                8.  alpine                              8.  volcanic\n",
      "\n",
      "        The third and fourth ELU digits are unique to the mapping unit \n",
      "        and have no special meaning to the climatic or geologic zones.\n",
      "\n",
      "Forest Cover Type Classes:\t1 -- Spruce/Fir\n",
      "                                2 -- Lodgepole Pine\n",
      "                                3 -- Ponderosa Pine\n",
      "                                4 -- Cottonwood/Willow\n",
      "                                5 -- Aspen\n",
      "                                6 -- Douglas-fir\n",
      "                                7 -- Krummholz\n",
      "\n",
      "\n",
      "8.  Basic Summary Statistics for quantitative variables only\n",
      "\t(whole dataset -- thanks to Phil Rennert for the summary values):\n",
      "\n",
      "Name                                    Units             Mean   Std Dev\n",
      "Elevation                               meters          2959.36  279.98\n",
      "Aspect                                  azimuth          155.65  111.91\n",
      "Slope                                   degrees           14.10    7.49\n",
      "Horizontal_Distance_To_Hydrology        meters           269.43  212.55\n",
      "Vertical_Distance_To_Hydrology          meters            46.42   58.30\n",
      "Horizontal_Distance_To_Roadways         meters          2350.15 1559.25\n",
      "Hillshade_9am                           0 to 255 index   212.15   26.77\n",
      "Hillshade_Noon                          0 to 255 index   223.32   19.77\n",
      "Hillshade_3pm                           0 to 255 index   142.53   38.27\n",
      "Horizontal_Distance_To_Fire_Points      meters          1980.29 1324.19\n",
      "\n",
      "\n",
      "9.\tMissing Attribute Values:  None.\n",
      "\n",
      "\n",
      "10.\tClass distribution:\n",
      "\n",
      "           Number of records of Spruce-Fir:                211840 \n",
      "           Number of records of Lodgepole Pine:            283301 \n",
      "           Number of records of Ponderosa Pine:             35754 \n",
      "           Number of records of Cottonwood/Willow:           2747 \n",
      "           Number of records of Aspen:                       9493 \n",
      "           Number of records of Douglas-fir:                17367 \n",
      "           Number of records of Krummholz:                  20510  \n",
      "           Number of records of other:                          0  \n",
      "\t\t\n",
      "           Total records:                                  581012\n",
      "\n",
      "=====================================================================\n",
      "Jock A. Blackard\n",
      "08/28/1998 -- original text\n",
      "12/07/1999 -- updated mailing address, citations, background info \n",
      "\t\t  for study area, added summary statistics.\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /home/shyeon/workspace/apache-project/advanced-spark/data/ch04/covtype.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2590,56,2,212,-6,390,220,235,151,6225,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2804,139,9,268,65,3180,234,238,135,6121,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "2785,155,18,242,118,3090,238,238,122,6211,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2\n",
      "2595,45,2,153,-1,391,220,234,150,6172,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2579,132,6,300,-15,67,230,237,140,6031,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "2606,45,7,270,5,633,222,225,138,6256,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2605,49,4,234,7,573,222,230,144,6228,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2617,45,9,240,56,666,223,221,133,6244,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2612,59,10,247,11,636,228,219,124,6230,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n"
     ]
    }
   ],
   "source": [
    "!head /home/shyeon/workspace/apache-project/advanced-spark/data/ch04/covtype.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|cover_type|\n",
      "+----------+\n",
      "|         5|\n",
      "|         5|\n",
      "|         2|\n",
      "|         2|\n",
      "|         5|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elevation                               quantitative    meters                       Elevation in meters\n",
    "# Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "# Slope                                   quantitative    degrees                      Slope in degrees\n",
    "# Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "# Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "# Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "# Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "# Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "# Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "# Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "# Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "# Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "# Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
    "\n",
    "wilderness_area_cols = [f\"wilderness_area_{i}\" for i in range(4)] # 황야 지역 (4 dummy variables)\n",
    "soil_type_cols = [f\"soil_type_{i}\" for i in range(40)] # 토양 유형 (40 dummy variables)\n",
    "\n",
    "schema = [\n",
    "    T.StructField(\"elevation\", T.DoubleType(), True),\n",
    "    T.StructField(\"aspect\", T.DoubleType(), True),\n",
    "    T.StructField(\"slope\", T.DoubleType(), True),\n",
    "    T.StructField(\"horz_dist_to_hydro\", T.DoubleType(), True), # 가장 가까운 지표수까지 거리\n",
    "    T.StructField(\"vert_dist_to_hydro\", T.DoubleType(), True), # 가장 가까운 지표수까지 거리\n",
    "    T.StructField(\"horz_dist_to_road\", T.DoubleType(), True), # 가장 가까운 도로까지 거리\n",
    "    T.StructField(\"hillshade_9am\", T.IntegerType(), True), # 언덕 그늘\n",
    "    T.StructField(\"hillshade_noon\", T.IntegerType(), True),\n",
    "    T.StructField(\"hillshade_3pm\", T.IntegerType(), True),\n",
    "    T.StructField(\"horz_dist_to_fire\", T.DoubleType(), True),\n",
    "]\n",
    "wilderness_area_schema = [T.StructField(col, T.IntegerType(), True) for col in wilderness_area_cols] \n",
    "soil_type_schema = [T.StructField(col, T.IntegerType(), True) for col in soil_type_cols] \n",
    "cover_type_schema = [T.StructField(\"cover_type\", T.IntegerType(), True)]\n",
    "\n",
    "schema.extend(wilderness_area_schema)\n",
    "schema.extend(soil_type_schema)\n",
    "schema.extend(cover_type_schema)\n",
    "schema = T.StructType(schema)\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"header\", False)\n",
    "    .option(\"sep\", \",\")\n",
    "    .schema(schema)\n",
    "    .load(\"/data/advanced-spark/ch04/covtype.data\")\n",
    ")\n",
    "\n",
    "wilderness_area_cols = [f\"wilderness_area_{i}\" for i in range(4)]\n",
    "soil_type_cols = [f\"soil_type_{i}\" for i in range(40)]\n",
    "\n",
    "df.select(\"cover_type\").show(5) # 모든 컬럼의 스키마가 반영되었는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                |label|\n",
      "+--------------------------------------------------------------------------------------------------------+-----+\n",
      "|(54,[0,1,2,3,5,6,7,8,9,10,42],[2596.0,51.0,3.0,258.0,510.0,221.0,232.0,148.0,6279.0,1.0,1.0])           |5.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,42],[2590.0,56.0,2.0,212.0,-6.0,390.0,220.0,235.0,151.0,6225.0,1.0,1.0])    |5.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,25],[2804.0,139.0,9.0,268.0,65.0,3180.0,234.0,238.0,135.0,6121.0,1.0,1.0])  |2.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,43],[2785.0,155.0,18.0,242.0,118.0,3090.0,238.0,238.0,122.0,6211.0,1.0,1.0])|2.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,42],[2595.0,45.0,2.0,153.0,-1.0,391.0,220.0,234.0,150.0,6172.0,1.0,1.0])    |5.0  |\n",
      "+--------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "transformer = RFormula(formula=\"cover_type ~ .\").fit(df)\n",
    "prepared_df = transformer.transform(df).select(\"features\", \"label\")\n",
    "prepared_df.show(5, False) # features와 label이 추가됨\n",
    "\n",
    "train_df, test_df = prepared_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (Spark Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featuresCol: features column name. (default: features, current: indexedFeatures)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name. (default: label, current: indexedLabel)\n",
      "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
      "predictionCol: prediction column name. (default: prediction, current: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: -2467889281745786366)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "+------------+----------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|indexedLabel|prediction|probability                                                                                                                    |\n",
      "+------------+----------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0         |0.0       |[0.7761148290485025,0.14158991518685396,0.014734627617280678,4.141266896368937E-5,0.01976212562947257,0.047757089848926584,0.0]|\n",
      "|0.0         |0.0       |[0.7761148290485025,0.14158991518685396,0.014734627617280678,4.141266896368937E-5,0.01976212562947257,0.047757089848926584,0.0]|\n",
      "|0.0         |0.0       |[0.7761148290485025,0.14158991518685396,0.014734627617280678,4.141266896368937E-5,0.01976212562947257,0.047757089848926584,0.0]|\n",
      "|0.0         |0.0       |[0.7761148290485025,0.14158991518685396,0.014734627617280678,4.141266896368937E-5,0.01976212562947257,0.047757089848926584,0.0]|\n",
      "|0.0         |0.0       |[0.7761148290485025,0.14158991518685396,0.014734627617280678,4.141266896368937E-5,0.01976212562947257,0.047757089848926584,0.0]|\n",
      "+------------+----------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.304675 \n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_01ef9d0fe2aa, depth=5, numNodes=51, numClasses=7, numFeatures=54\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(prepared_df)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "feature_indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=7).fit(prepared_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(train_df, test_df) = prepared_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\",\n",
    "                            featuresCol=\"indexedFeatures\",\n",
    "                            predictionCol=\"prediction\")\n",
    "print(dt.explainParams())\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "stages = [label_indexer, feature_indexer, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"indexedLabel\", \"prediction\", \"probability\").show(5, False)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\",\n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "#### Spark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71792.0</td>\n",
       "      <td>11589.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24524.0</td>\n",
       "      <td>37303.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1944.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8648.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.0</td>\n",
       "      <td>3341.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2736.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2073.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2975.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2545.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1       2       3    4     5      6\n",
       "0  71792.0  11589.0  1344.0   178.0  0.0  21.0   31.0\n",
       "1  24524.0  37303.0    55.0  1100.0  0.0   0.0    0.0\n",
       "2   1944.0      0.0  8648.0     0.0  0.0   0.0  232.0\n",
       "3     90.0   3341.0    20.0  2736.0  0.0   0.0    0.0\n",
       "4   2073.0      0.0  2975.0     0.0  0.0   0.0  176.0\n",
       "5   2545.0      0.0   225.0     0.0  0.0  33.0    0.0\n",
       "6      8.0      0.0   477.0     0.0  0.0   0.0  325.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# important: need to cast to float type, and order by prediction, else it won't work\n",
    "# select only prediction and label columns\n",
    "preds_and_labels = predictions.select('prediction','indexedLabel').orderBy('prediction') # 컬럼순서 주의(pred, real 순) \n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "cm = metrics.confusionMatrix().toArray() # python list\n",
    "pd.DataFrame(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = (\n",
    "    predictions.select('prediction', 'indexedLabel')\n",
    "    .groupby(\"indexedLabel\")\n",
    "    .pivot(\"prediction\", list(range(0, 7)))\n",
    "    .count()\n",
    "    .fillna(0)\n",
    "    .sort(\"indexedLabel\")\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indexedLabel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>71792</td>\n",
       "      <td>11589</td>\n",
       "      <td>1344</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>24524</td>\n",
       "      <td>37303</td>\n",
       "      <td>55</td>\n",
       "      <td>1100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>8648</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>90</td>\n",
       "      <td>3341</td>\n",
       "      <td>20</td>\n",
       "      <td>2736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>2975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>2545</td>\n",
       "      <td>0</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1     2     3  4   5    6\n",
       "indexedLabel                                      \n",
       "0.0           71792  11589  1344   178  0  21   31\n",
       "1.0           24524  37303    55  1100  0   0    0\n",
       "2.0            1944      0  8648     0  0   0  232\n",
       "3.0              90   3341    20  2736  0   0    0\n",
       "4.0            2073      0  2975     0  0   0  176\n",
       "5.0            2545      0   225     0  0  33    0\n",
       "6.0               8      0   477     0  0   0  325"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = cm.reindex(cm[\"indexedLabel\"], axis=0).drop(\"indexedLabel\", axis=1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "#               .addGrid(dt.impurity, [\"gini\", \"entropy\"])\n",
    "              .addGrid(dt.impurity, [\"entropy\"])\n",
    "              .addGrid(dt.maxDepth, [30]) # <= 30\n",
    "              .addGrid(dt.maxBins, range(400, 600, 10))\n",
    "              .addGrid(dt.minInfoGain, [0.0])\n",
    "              .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\",\n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\", )\n",
    "\n",
    "validator = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, parallelism=1, seed=42)\n",
    "validated_model = validator.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_model.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(validated_model.bestModel.transform(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- onehot으로 분리된 열들을 한 열로 통함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(F.array(*wilderness_area_cols)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
